{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2019-02-24:02:12:00 __init__.py: 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format='[%(levelname)s] %(asctime)s %(filename)s: %(lineno)d: %(message)s',\n",
    "    datefmt='%Y-%m-%d:%H:%M:%S',\n",
    "    level=logging.DEBUG)\n",
    "\n",
    "DATE_TODAY = dt(2019, 1, 26)\n",
    "\n",
    "FEATS_EXCLUDED = [\n",
    "    'first_active_month', 'target', 'card_id', 'outliers',\n",
    "    'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "    'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "    'OOF_PRED', 'month_0',\n",
    "# my adding\n",
    " 'days_feature_1_ratio',\n",
    " 'days_feature_2_ratio',\n",
    " 'days_feature_3_ratio',\n",
    " 'features_max',\n",
    " 'new_subsector_id_nunique',\n",
    " 'new_merchant_id_nunique',\n",
    " 'new_merchant_category_id_nunique',\n",
    " 'new_hour_nunique',\n",
    " 'new_hour_mean',\n",
    " 'new_hour_max',\n",
    " 'new_weekofyear_nunique',\n",
    " 'new_weekday_mean',\n",
    " 'new_weekday_min',\n",
    " 'new_day_nunique',\n",
    " 'new_day_min',\n",
    " 'new_purchase_amount_sum',\n",
    " 'new_purchase_amount_min',\n",
    " 'new_purchase_amount_var',\n",
    " 'new_purchase_amount_skew',\n",
    " 'new_installments_sum',\n",
    " 'new_installments_max',\n",
    " 'new_installments_mean',\n",
    " 'new_installments_var',\n",
    " 'new_installments_skew',\n",
    " 'new_month_lag_var',\n",
    " 'new_month_lag_skew',\n",
    " 'new_month_diff_var',\n",
    " 'new_month_diff_skew',\n",
    " 'new_weekend_mean',\n",
    " 'new_category_2_mean',\n",
    " 'new_category_3_mean',\n",
    " 'new_card_id_count',\n",
    " 'new_price_mean',\n",
    " 'new_price_max',\n",
    " 'new_price_var',\n",
    " 'new_duration_var',\n",
    " 'new_duration_skew',\n",
    " 'new_amount_month_ratio_mean',\n",
    " 'new_amount_month_ratio_min',\n",
    " 'new_amount_month_ratio_var',\n",
    " 'new_amount_month_ratio_skew',\n",
    " 'new_category_2_mean_mean',\n",
    " 'new_CLV',\n",
    " 'hist_month_mean',\n",
    " 'hist_hour_mean',\n",
    " 'hist_hour_max',\n",
    " 'hist_weekofyear_mean',\n",
    " 'hist_weekday_mean',\n",
    " 'hist_day_mean',\n",
    " 'hist_day_min',\n",
    " 'hist_purchase_amount_var',\n",
    " 'hist_purchase_amount_skew',\n",
    " 'hist_installments_skew',\n",
    " 'hist_month_lag_skew',\n",
    " 'hist_month_diff_skew',\n",
    " 'hist_weekend_mean',\n",
    " 'hist_category_2_mean',\n",
    " 'hist_Mothers_Day_2018_mean',\n",
    " 'hist_duration_var',\n",
    " 'hist_duration_skew',\n",
    " 'hist_amount_month_ratio_max',\n",
    " 'hist_amount_month_ratio_var',\n",
    " 'hist_amount_month_ratio_skew',\n",
    " 'hist_category_2_mean_mean',\n",
    " 'hist_purchase_date_average',\n",
    " 'hist_first_buy',\n",
    " 'new_first_buy',\n",
    " 'card_id_total',\n",
    " 'card_id_cnt_total',\n",
    " 'purchase_amount_total',\n",
    " 'purchase_amount_mean',\n",
    " 'purchase_amount_max',\n",
    " 'purchase_amount_min',\n",
    " 'purchase_amount_ratio',\n",
    " 'installments_mean',\n",
    " 'installments_max',\n",
    " 'installments_ratio',\n",
    " 'price_total',\n",
    " 'price_mean',\n",
    " 'price_max',\n",
    " 'duration_max',\n",
    " 'amount_month_ratio_mean',\n",
    " 'amount_month_ratio_min',\n",
    " 'amount_month_ratio_max'\n",
    "]\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "        by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = df.columns.tolist()\n",
    "\n",
    "    categorical_columns = list(filter(lambda c: c in ['object'], df.dtypes))\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "\n",
    "    new_columns = list(filter(lambda c: c not in original_columns, df.columns))\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "def process_main_df(df):\n",
    "    \n",
    "#     # datetime features\n",
    "#     df['quarter'] = df['first_active_month'].dt.quarter\n",
    "#     df['elapsed_time'] = (DATE_TODAY - df['first_active_month']).dt.days\n",
    "\n",
    "#     feature_cols = ['feature_1', 'feature_2', 'feature_3']\n",
    "#     for f in feature_cols:    \n",
    "#         df['days_' + f] = df['elapsed_time'] * df[f]\n",
    "#         df['days_' + f + '_ratio'] = df[f] / df['elapsed_time']\n",
    "\n",
    "#     # one hot encoding\n",
    "#     df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "#     df_feats = df.reindex(columns=feature_cols)\n",
    "#     df['features_sum'] = df_feats.sum(axis=1)\n",
    "#     df['features_mean'] = df_feats.mean(axis=1)\n",
    "#     df['features_max'] = df_feats.max(axis=1)\n",
    "#     df['features_min'] = df_feats.min(axis=1)\n",
    "#     df['features_var'] = df_feats.std(axis=1)\n",
    "#     df['features_prod'] = df_feats.product(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# preprocessing train & test\n",
    "def train_test(num_rows=None):\n",
    "\n",
    "    def read_csv(filename):\n",
    "        df = pd.read_csv(\n",
    "            filename, index_col=['card_id'], parse_dates=['first_active_month'], nrows=num_rows)\n",
    "        return df\n",
    "    \n",
    "    # load csv\n",
    "    train_df = read_csv('../input/train.csv')\n",
    "    test_df = read_csv('../input/test.csv') \n",
    "    logger.info(\"samples: train {}, test: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30., 'outliers'] = 1\n",
    "\n",
    "    train_df = reduce_mem_usage(process_main_df(train_df))\n",
    "    test_df = reduce_mem_usage(process_main_df(test_df))\n",
    "\n",
    "    feature_cols = ['feature_1', 'feature_2', 'feature_3']\n",
    "    for f in feature_cols:\n",
    "        order_label = train_df.groupby([f])['outliers'].mean()\n",
    "        train_df[f] = train_df[f].map(order_label)\n",
    "        test_df[f] = test_df[f].map(order_label)    \n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def process_date(df):\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['day'] = df['purchase_date'].dt.day\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['weekday'] = df['purchase_date'].dt.weekday\n",
    "    df['weekend'] = (df['purchase_date'].dt.weekday >= 5).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def dist_holiday(df, col_name, date_holiday, date_ref, period=100):\n",
    "    df[col_name] = np.maximum(np.minimum((pd.to_datetime(date_holiday) - df[date_ref]).dt.days, period), 0)\n",
    "\n",
    "\n",
    "def historical_transactions(num_rows=None):\n",
    "    \"\"\"\n",
    "    preprocessing historical transactions\n",
    "    \"\"\"\n",
    "    na_dict = {\n",
    "        'category_2': 1.,\n",
    "        'category_3': 'A',\n",
    "        'merchant_id': 'M_ID_00a6ca8a8a',\n",
    "    }\n",
    "\n",
    "    holidays = [\n",
    "        # ('Christmas_Day_2017', '2017-12-25'),  # Christmas: December 25 2017\n",
    "        # ('Mothers_Day_2017', '2017-06-04'),  # Mothers Day: May 14 2017\n",
    "        # ('fathers_day_2017', '2017-08-13'),  # fathers day: August 13 2017\n",
    "        # ('Children_day_2017', '2017-10-12'),  # Childrens day: October 12 2017\n",
    "        # ('Valentine_Day_2017', '2017-06-12'),  # Valentine's Day : 12th June, 2017\n",
    "        # ('Black_Friday_2017', '2017-11-24'),  # Black Friday: 24th November 2017\n",
    "        # ('Mothers_Day_2018', '2018-05-13'),\n",
    "    ]\n",
    "\n",
    "    # agg\n",
    "    aggs = dict()\n",
    "    col_unique = ['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    aggs.update({col: ['nunique'] for col in col_unique})\n",
    "\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "    aggs.update({col: ['nunique', 'mean', 'min', 'max'] for col in col_seas})\n",
    "\n",
    "    aggs_specific = {\n",
    "        'purchase_amount': ['sum', 'max', 'min', 'mean', 'var', 'skew'],\n",
    "        'installments': ['sum', 'max', 'mean', 'var', 'skew'],\n",
    "        'purchase_date': ['max', 'min'],\n",
    "        'month_lag': ['max', 'min', 'mean', 'var', 'skew'],\n",
    "        'month_diff': ['max', 'min', 'mean', 'var', 'skew'],\n",
    "        'authorized_flag': ['mean'],\n",
    "        'weekend': ['mean'], # overwrite\n",
    "        'weekday': ['mean'], # overwrite\n",
    "        'day': ['nunique', 'mean', 'min'], # overwrite\n",
    "        'category_1': ['mean'],\n",
    "        'category_2': ['mean'],\n",
    "        'category_3': ['mean'],\n",
    "        'card_id': ['size', ],\n",
    "        'price': ['sum', 'mean', 'max', 'min', 'var'],\n",
    "        # 'Christmas_Day_2017': ['mean', 'sum'],\n",
    "        # 'Mothers_Day_2017': ['mean', 'sum'],\n",
    "        # 'fathers_day_2017': ['mean', 'sum'],\n",
    "        # 'Children_day_2017': ['mean', 'sum'],\n",
    "        # 'Valentine_Day_2017': ['mean', 'sum'],\n",
    "        # 'Black_Friday_2017': ['mean', 'sum'],\n",
    "        # 'Mothers_Day_2018': ['mean', 'sum'],\n",
    "        # 'duration': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "        # 'amount_month_ratio': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "    }\n",
    "    aggs.update(aggs_specific)\n",
    "\n",
    "    # starting to process\n",
    "    # load csv\n",
    "    df = pd.read_csv('../input/historical_transactions.csv', nrows=num_rows)\n",
    "    logger.info('read historical_transactions {}'.format(df.shape))\n",
    "    \n",
    "    # fillna\n",
    "    df.fillna(na_dict, inplace=True)\n",
    "    df['installments'].replace({\n",
    "        -1: np.nan, 999: np.nan}, inplace=True)\n",
    "\n",
    "    # trim\n",
    "    # df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(np.int16)\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0}).astype(np.int16)\n",
    "    df['category_3'] = df['category_3'].map({'A': 0, 'B': 1, 'C':2}).astype(np.int16)\n",
    "\n",
    "    # additional features\n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "\n",
    "    # datetime features\n",
    "    df = process_date(df)\n",
    "\n",
    "    # holidays\n",
    "    for d_name, d_day in holidays:\n",
    "        dist_holiday(df, d_name, d_day, 'purchase_date')\n",
    "\n",
    "    df['month_diff'] = (DATE_TODAY - df['purchase_date']).dt.days // 30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    # df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    # df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    for col in ['category_2', 'category_3']:\n",
    "        df[col + '_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        df[col + '_min'] = df.groupby([col])['purchase_amount'].transform('min')\n",
    "        df[col + '_max'] = df.groupby([col])['purchase_amount'].transform('max')\n",
    "        df[col + '_sum'] = df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col + '_mean'] = ['mean']\n",
    "    \n",
    "    df = df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    df.columns = pd.Index([e[0] + \"_\" + e[1] for e in df.columns.tolist()])\n",
    "    df.columns = ['hist_' + c for c in df.columns]\n",
    "\n",
    "    # df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "\n",
    "    df['hist_purchase_date_diff'] = (df['hist_purchase_date_max'] - df['hist_purchase_date_min']).dt.days\n",
    "    df['hist_purchase_date_average'] = df['hist_purchase_date_diff'] / df['hist_card_id_size']\n",
    "    df['hist_purchase_date_uptonow'] = (DATE_TODAY - df['hist_purchase_date_max']).dt.days\n",
    "    df['hist_purchase_date_uptomin'] = (DATE_TODAY - df['hist_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def new_merchant_transactions(num_rows=None):\n",
    "    \"\"\"\n",
    "    preprocessing new_merchant_transactions\n",
    "    \"\"\"\n",
    "    na_dict = {\n",
    "        'category_2': 1.,\n",
    "        'category_3': 'A',\n",
    "        'merchant_id': 'M_ID_00a6ca8a8a',\n",
    "    }\n",
    "\n",
    "    holidays = [\n",
    "        # ('Christmas_Day_2017', '2017-12-25'),  # Christmas: December 25 2017\n",
    "        # # ('Mothers_Day_2017', '2017-06-04'),  # Mothers Day: May 14 2017\n",
    "        # # ('fathers_day_2017', '2017-08-13'),  # fathers day: August 13 2017\n",
    "        # ('Children_day_2017', '2017-10-12'),  # Childrens day: October 12 2017\n",
    "        # # ('Valentine_Day_2017', '2017-06-12'),  # Valentine's Day : 12th June, 2017\n",
    "        # ('Black_Friday_2017', '2017-11-24'),  # Black Friday: 24th November 2017\n",
    "        # ('Mothers_Day_2018', '2018-05-13'),\n",
    "    ]\n",
    "    \n",
    "    aggs = dict()\n",
    "    col_unique = ['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    aggs.update({col: ['nunique'] for col in col_unique})\n",
    "\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "    aggs.update({col: ['nunique', 'mean', 'min', 'max'] for col in col_seas})\n",
    "\n",
    "    aggs_specific = {\n",
    "        'purchase_amount': ['sum', 'max', 'min', 'mean', 'var', 'skew'],\n",
    "        'installments': ['sum', 'max', 'mean', 'var', 'skew'],\n",
    "        'purchase_date': ['max', 'min'],\n",
    "        'month_lag': ['max', 'min', 'mean', 'var', 'skew'],\n",
    "        'month_diff': ['mean', 'var', 'skew'],\n",
    "        'weekend': ['mean'],\n",
    "        'month': ['mean', 'min', 'max'],\n",
    "        'weekday': ['mean', 'min', 'max'],\n",
    "        'category_1': ['mean'],\n",
    "        'category_2': ['mean'],\n",
    "        'category_3': ['mean'],\n",
    "        'card_id': ['size', 'count'],\n",
    "        'price': ['mean', 'max', 'min', 'var'],\n",
    "        # 'Christmas_Day_2017': ['mean', 'sum'],\n",
    "        # 'Children_day_2017': ['mean', 'sum'],\n",
    "        # 'Black_Friday_2017': ['mean', 'sum'],\n",
    "        # 'Mothers_Day_2018': ['mean', 'sum'],\n",
    "        # 'duration': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "        # 'amount_month_ratio': ['mean', 'min', 'max', 'var', 'skew'],\n",
    "    }\n",
    "    aggs.update(aggs_specific)\n",
    "\n",
    "    # load csv\n",
    "    df = pd.read_csv('../input/new_merchant_transactions.csv', nrows=num_rows)\n",
    "    logger.info('read new_merchant_transactions {}'.format(df.shape))\n",
    "    \n",
    "    # fillna\n",
    "    df.fillna(na_dict, inplace=True)\n",
    "    df['installments'].replace({\n",
    "        -1: np.nan, 999: np.nan}, inplace=True)\n",
    "\n",
    "    # trim\n",
    "    # df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int).astype(np.int16)\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0}).astype(int).astype(np.int16)\n",
    "    df['category_3'] = df['category_3'].map({'A': 0, 'B': 1, 'C': 2}).astype(int).astype(np.int16)\n",
    "\n",
    "    # # additional features\n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "\n",
    "    # datetime features\n",
    "    df = process_date(df)\n",
    "    for d_name, d_day in holidays:\n",
    "        dist_holiday(df, d_name, d_day, 'purchase_date')\n",
    "\n",
    "    df['month_diff'] = (DATE_TODAY - df['purchase_date']).dt.days // 30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    # df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    # df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    for col in ['category_2', 'category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        df[col+'_min'] = df.groupby([col])['purchase_amount'].transform('min')\n",
    "        df[col+'_max'] = df.groupby([col])['purchase_amount'].transform('max')\n",
    "        df[col+'_sum'] = df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col + '_mean'] = ['mean']\n",
    "\n",
    "    df = df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    df.columns = pd.Index([e[0] + \"_\" + e[1] for e in df.columns.tolist()])\n",
    "    df.columns = ['new_' + c for c in df.columns]\n",
    "\n",
    "    # df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    \n",
    "    df['new_purchase_date_diff'] = (df['new_purchase_date_max'] - df['new_purchase_date_min']).dt.days\n",
    "    df['new_purchase_date_average'] = df['new_purchase_date_diff'] / df['new_card_id_size']\n",
    "    df['new_purchase_date_uptonow'] = (DATE_TODAY - df['new_purchase_date_max']).dt.days\n",
    "    df['new_purchase_date_uptomin'] = (DATE_TODAY - df['new_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df\n",
    "        \n",
    "\n",
    "# additional features\n",
    "def additional_features(df):\n",
    "    \n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features = [\n",
    "        'hist_purchase_date_max', 'hist_purchase_date_min', 'new_purchase_date_max', 'new_purchase_date_min']\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    #\n",
    "    # df['card_id_total'] = df['new_card_id_size'] + df['hist_card_id_size']\n",
    "    # df['card_id_cnt_total'] = df['new_card_id_count'] + df['hist_card_id_count']\n",
    "    # df['card_id_cnt_ratio'] = df['new_card_id_count'] / df['hist_card_id_count']\n",
    "    \n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum'] + df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean'] + df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max'] + df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min'] + df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum'] / df['hist_purchase_amount_sum']\n",
    "\n",
    "    df['installments_total'] = df['new_installments_sum'] + df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean'] + df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max'] + df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum'] / df['hist_installments_sum']\n",
    "\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "\n",
    "    #\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean'] + df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean'] / df['hist_month_diff_mean']\n",
    "    \n",
    "    df['month_lag_mean'] = df['new_month_lag_mean'] + df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max'] + df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min'] + df['hist_month_lag_min']\n",
    "    # df['category_1_mean'] = df['new_category_1_mean'] + df['hist_category_1_mean']\n",
    "        \n",
    "    # df['duration_mean'] = df['new_duration_mean'] + df['hist_duration_mean']\n",
    "    # df['duration_min'] = df['new_duration_min'] + df['hist_duration_min']\n",
    "    # df['duration_max'] = df['new_duration_max'] + df['hist_duration_max']\n",
    "    \n",
    "    # df['amount_month_ratio_mean'] = df['new_amount_month_ratio_mean'] + df['hist_amount_month_ratio_mean']\n",
    "    # df['amount_month_ratio_min'] = df['new_amount_month_ratio_min'] + df['hist_amount_month_ratio_min']\n",
    "    # df['amount_month_ratio_max'] = df['new_amount_month_ratio_max'] + df['hist_amount_month_ratio_max']\n",
    "    \n",
    "    # df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "    # df['CLV_sq'] = df['new_CLV'] * df['hist_CLV']\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "ecbb5149328b361993ff4a0f13b662e2d3bd327c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-02-24:02:13:21 <ipython-input-1-5acfe8893eee>: 323: read historical_transactions (29112361, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1582.53 MB\n",
      "Decreased by 63.9%\n",
      "Memory usage after optimization is: 48.12 MB\n",
      "Decreased by 55.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-02-24:02:23:45 <ipython-input-1-5acfe8893eee>: 436: read new_merchant_transactions (1963031, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 102.97 MB\n",
      "Decreased by 65.2%\n",
      "Memory usage after optimization is: 38.44 MB\n",
      "Decreased by 56.0%\n"
     ]
    }
   ],
   "source": [
    "debug=False\n",
    "num_rows = 10000 if debug else None\n",
    "#  with timer(\"historical transactions\"):\n",
    "hist_df = historical_transactions(num_rows)\n",
    "# with timer(\"new merchants\"):\n",
    "new_merchant_df = new_merchant_transactions(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "185036f42263f92a04b44de367f7bdcbeb6c7a51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-02-24:02:27:49 <ipython-input-1-5acfe8893eee>: 232: samples: train (201917, 5), test: (123623, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 4.24 MB\n",
      "Decreased by 60.7%\n",
      "Memory usage after optimization is: 2.24 MB\n",
      "Decreased by 52.5%\n",
      "Memory usage after optimization is: 73.17 MB\n",
      "Decreased by 33.6%\n",
      "Memory usage after optimization is: 43.98 MB\n",
      "Decreased by 34.4%\n"
     ]
    }
   ],
   "source": [
    "# with timer(\"additional features\"):\n",
    "df = pd.concat([new_merchant_df, hist_df], axis=1)\n",
    "del new_merchant_df, hist_df\n",
    "gc.collect()\n",
    "\n",
    "train_df, test_df = train_test(num_rows)\n",
    "train_df = train_df.join(df, how='left', on='card_id')\n",
    "test_df = test_df.join(df, how='left', on='card_id')\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "train_df = additional_features(train_df)\n",
    "test_df = additional_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"processed_train_df_v1.csv\")\n",
    "test_df.to_csv(\"processed_test_df_v1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
